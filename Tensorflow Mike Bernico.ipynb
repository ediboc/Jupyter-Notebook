{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Multinomial Logistic Regression with Tensorflow from Youtube](https://www.youtube.com/watch?v=2JiXktBn_2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#image tools\n",
    "import matplotlib.pyplot as plt\n",
    "import image\n",
    "\n",
    "#filesystem tools\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(prediction,labels):\n",
    "    return (100.0 * (np.sum(np.equal(np.argmax(prediction, 1), np.argmax(labels, 1))))\n",
    "           /prediction.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Variables\n",
    "    batch_size = 128\n",
    "    beta = .01\n",
    "    image_size = 28\n",
    "    num_labels = 10\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(mnist.validation.images)\n",
    "    tf_test_dataset = tf.constant(mnist.test.images)\n",
    "    \n",
    "    # Weights and biases for output/logit layer\n",
    "    w_logit = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels])) \n",
    "    b_logit = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def model(data):\n",
    "        \"\"\"\n",
    "        Assembles the NN\n",
    "        \"\"\"\n",
    "        return tf.matmul(data, w_logit) + b_logit\n",
    "                          \n",
    "    # Training Computations\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits,labels= tf_train_labels))\n",
    "    regularized_loss = tf.nn.l2_loss(w_logit)\n",
    "    total_loss = loss + beta + regularized_loss\n",
    "    \n",
    "    # Optimizer:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11.867926\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 25: 8.110176\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 50: 6.076201\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 75: 4.282178\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 100: 3.416584\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 14.3%\n",
      "Minibatch loss at step 125: 2.860052\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 18.8%\n",
      "Minibatch loss at step 150: 2.267735\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 25.2%\n",
      "Minibatch loss at step 175: 1.965567\n",
      "Minibatch accuracy: 30.5%\n",
      "Validation accuracy: 31.7%\n",
      "Minibatch loss at step 200: 1.808925\n",
      "Minibatch accuracy: 41.4%\n",
      "Validation accuracy: 39.1%\n",
      "Minibatch loss at step 225: 1.710536\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 48.1%\n",
      "Minibatch loss at step 250: 1.747340\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 275: 1.659518\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 63.1%\n",
      "Minibatch loss at step 300: 1.646910\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 325: 1.616999\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 350: 1.664983\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 375: 1.658908\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 400: 1.645909\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 425: 1.620416\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 450: 1.605623\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 475: 1.646515\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 500: 1.684431\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 79.8%\n",
      "W: [[ -1.13148130e-02  -3.58303187e-05  -3.02754436e-03 ...,  -4.71572531e-03\n",
      "    6.43638987e-03   1.25972200e-02]\n",
      " [  5.52304927e-03  -1.69855019e-03   4.58615925e-03 ...,  -3.42400442e-03\n",
      "   -2.36767856e-03   3.75084742e-03]\n",
      " [ -2.39297142e-03   6.94546103e-03   3.84599133e-03 ...,   1.30193739e-03\n",
      "    9.23859421e-03   3.68217961e-03]\n",
      " ..., \n",
      " [ -1.82558235e-03   2.93087959e-03   8.32261518e-03 ...,  -1.28590432e-03\n",
      "   -4.50031646e-03   1.21719716e-02]\n",
      " [ -2.26318254e-03   3.20625887e-03  -5.03947400e-03 ...,   5.54842502e-03\n",
      "    8.73384904e-03   2.91201402e-03]\n",
      " [  2.76474655e-03   8.94565787e-03  -4.41120379e-03 ...,  -2.54061236e-03\n",
      "   -2.45083286e-03  -3.08754435e-03]]\n",
      "B: [-0.00285368  0.0913696  -0.05473193  0.04847762  0.09809796  0.02942116\n",
      " -0.05864837 -0.10546921 -0.00943494 -0.03622818]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        #Generate a minibatch.\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "        #Dictionary to feed\n",
    "        feed_dict = {tf_train_dataset: batch_data,tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict) \n",
    "    \n",
    "        if (step % 25 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), mnist.validation.labels))\n",
    "            \n",
    "    W_val = session.run(w_logit)\n",
    "    b_val = session.run(b_logit)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), mnist.test.labels))\n",
    "    print('W:',W_val)\n",
    "    print('B:',b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Multinomial Perceptrons in TensorFlow](https://www.youtube.com/watch?v=fQ8q8LTMzwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Variables\n",
    "    batch_size = 128\n",
    "    beta = .01\n",
    "    image_size = 28\n",
    "    num_labels = 10\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    #new\n",
    "    h1_size = 512 #number of neurons in hidden layer\n",
    "    keep_prob = 0.5 #probability of dropout\n",
    "    \n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(mnist.validation.images)\n",
    "    tf_test_dataset = tf.constant(mnist.test.images)\n",
    "    \n",
    "    # Weights and biases for hidden layer 1\n",
    "    w_h1 = tf.Variable(tf.truncated_normal([image_size*image_size, h1_size])) \n",
    "    b_h1 = tf.Variable(tf.zeros([h1_size]))\n",
    "    \n",
    "    # Weights and biases for output/logit layer\n",
    "    w_logit = tf.Variable(tf.truncated_normal([h1_size, num_labels])) \n",
    "    b_logit = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #keep probability for dropout. This is a variable so it can be turned on for train and off for predict\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    \n",
    "    def model(data):\n",
    "        \"\"\"\n",
    "        Assembles the NN\n",
    "\n",
    "        \"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(data, w_h1) + b_h1)\n",
    "        h1_drop_out = tf.nn.dropout(h1,keep_prob)\n",
    "        return tf.matmul(h1_drop_out, w_logit) + b_logit\n",
    "                          \n",
    "    # Training Computations\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits,labels= tf_train_labels))\n",
    "    regularized_loss = tf.nn.l2_loss(w_logit)\n",
    "    total_loss = loss + beta + regularized_loss\n",
    "    \n",
    "    # Optimizer:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 301.676483\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 5.8%\n",
      "Minibatch loss at step 25: 118.703156\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 29.2%\n",
      "Minibatch loss at step 50: 65.175095\n",
      "Minibatch accuracy: 30.5%\n",
      "Validation accuracy: 52.1%\n",
      "Minibatch loss at step 75: 36.946068\n",
      "Minibatch accuracy: 39.1%\n",
      "Validation accuracy: 64.2%\n",
      "Minibatch loss at step 100: 21.793938\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 125: 13.897432\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 150: 7.452451\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 175: 8.250422\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 200: 4.189260\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 225: 3.765678\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 250: 2.381036\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 275: 2.179690\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 300: 1.410209\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 85.9%\n",
      "Test accuracy: 86.2%\n",
      "W: [[ 0.06154916 -0.04954842 -0.01466208 ...,  0.02266478 -0.05263893\n",
      "  -0.00585941]\n",
      " [-0.02477599  0.01153914  0.00056311 ..., -0.01210789  0.00214629\n",
      "  -0.05091946]\n",
      " [-0.00792572  0.03053578 -0.04202378 ...,  0.0186286   0.02495872\n",
      "  -0.04809863]\n",
      " ..., \n",
      " [-0.02126146 -0.04569266 -0.01587002 ...,  0.09109512 -0.02509977\n",
      "  -0.02338214]\n",
      " [-0.0231704  -0.00438378  0.04108047 ..., -0.01342789  0.03912601\n",
      "   0.00104353]\n",
      " [ 0.06077282 -0.03481685  0.00530119 ..., -0.04769535 -0.06352542\n",
      "   0.01404103]]\n",
      "B: [ -8.92553572e-03   2.52738968e-02  -2.37189066e-02   2.02330175e-05\n",
      "  -2.62089801e-04   4.35755448e-03   1.13618504e-02   6.32821210e-03\n",
      "  -1.77169982e-02   3.28178331e-03]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 301\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        #Generate a minibatch.\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "        #Dictionary to feed\n",
    "        feed_dict = {tf_train_dataset: batch_data,tf_train_labels: batch_labels, keep_prob: 0.5}\n",
    "        _, l, predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict) \n",
    "    \n",
    "        if (step % 25 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(feed_dict={keep_prob:1.0}), \n",
    "                                                           mnist.validation.labels.astype(float)))\n",
    "            #feed_dict={keep_prob:1.0}\n",
    "    W_val = session.run(w_logit)\n",
    "    b_val = session.run(b_logit)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict={keep_prob:1.0}), mnist.test.labels))\n",
    "    print('W:',W_val)\n",
    "    print('B:',b_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [Convolutional Neural Networks in Tensorflow](https://www.youtube.com/watch?v=lTFOw8-P02Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Variables\n",
    "    batch_size = 128\n",
    "    beta = .01\n",
    "    image_size = 28\n",
    "    num_labels = 10\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    #second part\n",
    "    h1_size = 1024 #number of neurons in hidden layer\n",
    "    keep_prob = 0.5 #probability of dropout\n",
    "    \n",
    "    #new \n",
    "    patch_size = 5\n",
    "    num_channels = 1 #grayscale\n",
    "    depth = 16\n",
    "    \n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(x,W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size,image_size,num_channels ))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(mnist.validation.images.reshape(-1,28,28,1))\n",
    "    tf_test_dataset = tf.constant(mnist.test.images.reshape(-1,28,28,1))\n",
    "    \n",
    "    # Weights and biases \n",
    "    w_conv1 = weight_variable([5,5,1,32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    w_conv2 = weight_variable([5,5,32,64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    w_fc1 = weight_variable([7 * 7  * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    w_fc2 = weight_variable([1024,10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    #keep probability for dropout. This is a variable so it can be turned on for train and off for predict\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    def model(data):\n",
    "        \"\"\"\n",
    "        Assembles the NN\n",
    "\n",
    "        \"\"\"\n",
    "        h_conv1 = tf.nn.relu(conv2d(data, w_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        h_pool2_flat = tf.reshape(h_pool2,[-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "        return tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "                          \n",
    "    # Training Computations\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits,labels= tf_train_labels))\n",
    "    \n",
    "    # Optimizer:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.344798\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 16.8%\n",
      "Minibatch loss at step 100: 2.274041\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 28.9%\n",
      "Minibatch loss at step 200: 2.168340\n",
      "Minibatch accuracy: 30.5%\n",
      "Validation accuracy: 39.5%\n",
      "Minibatch loss at step 300: 2.072723\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 51.0%\n",
      "Minibatch loss at step 400: 1.953208\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 59.9%\n",
      "Minibatch loss at step 500: 1.861571\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 63.6%\n",
      "Minibatch loss at step 600: 1.931832\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 700: 1.909201\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 800: 1.812298\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 900: 1.872097\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 1000: 1.763211\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 80.1%\n",
      "W: [[ 0.09814602  0.06555619  0.01554624 ..., -0.0336039   0.0695828\n",
      "   0.05031287]\n",
      " [-0.00256959 -0.06705269 -0.14314273 ..., -0.10589147  0.03465383\n",
      "  -0.0122848 ]\n",
      " [ 0.02059156 -0.11717846 -0.00562595 ...,  0.18628773  0.01884233\n",
      "   0.00157707]\n",
      " ..., \n",
      " [-0.06250921  0.10485761  0.02008475 ..., -0.13285603  0.09765169\n",
      "   0.0365874 ]\n",
      " [-0.06344235 -0.15922543  0.09726617 ..., -0.04975387 -0.0498969\n",
      "   0.00285373]\n",
      " [-0.10825311  0.03749675  0.05904203 ..., -0.07186483 -0.03196305\n",
      "   0.02004747]]\n",
      "B: [ 0.099201    0.1068074   0.09715442  0.09801424  0.10270071  0.09961148\n",
      "  0.10255963  0.10022108  0.09526694  0.0984631 ]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        #Generate a minibatch.\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        #reshape\n",
    "        batch_data = batch_data.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "        #Dictionary to feed\n",
    "        feed_dict = {tf_train_dataset: batch_data,tf_train_labels: batch_labels, keep_prob: 0.5}\n",
    "        _, l, predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict) \n",
    "    \n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(feed_dict={keep_prob:1.0}), \n",
    "                                                           mnist.validation.labels.astype(float)))\n",
    "            #feed_dict={keep_prob:1.0}\n",
    "    W_val = session.run(w_fc2)\n",
    "    b_val = session.run(b_fc2)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict={keep_prob:1.0}), mnist.test.labels))\n",
    "    print('W:',W_val)\n",
    "    print('B:',b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
